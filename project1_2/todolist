Watch video to have a overview of what's gonna be done.
	Use piping to control the flow: cat input | java mapper | sort | java reducer > output
	Use javac command to compile .java to .class
	Use jar command to generate .jar. jar -cvf Result.jar file1 file2..
	Test program locally using a sample dataset before deploying to the EMR. No data structure to keep variables.
	Create EMR cluster.
	Use -cp option to sepecify which class you need to call. java -cp xxx.jar xxx.class
	When setting up EMR, output path should not exist before running the cluster. The location of jar files should be specified in Arguments. -files [S3 location of jar]
	
	1. Watch project primer to learn about S3 container
	2. Write Java code
	3. Test Java code locally, using project1.1's dataset
	4. Create cluster
	5. Deploy
	
1. How files are stored? -> refer to piazza
2. How to sort entried in different files -> Hadoop will do that for us
3. Coding Mapper & Reducer --> done
4. Grab data for local test
4. Test locally
5. EMR use hadoop. It's command is based on hadoop streaming.
6. Take a look at runner.sh
7. Complete runner.sh
8. Test runner.sh
9. Submit!